# -*- coding: utf-8 -*-
"""PyCon Portugal-Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K6xq8xiKW6Y219z8k4ZSRoKAHGSGWGrK

### `Importing Libraries`
"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
import xgboost as xgb
from imblearn.over_sampling import SMOTE

"""### `Loading the dataset: German Credit Risk  Dataset`"""

# Loading the dataset
data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data',
                   header=None, sep=' ')

# Define feature names
feature_names = ['status', 'duration', 'credit_history', 'purpose', 'amount',
                 'savings', 'employment_duration', 'installment_rate', 'statussex',
                 'other_debtors', 'residence_since', 'property', 'age', 'other_installment_plans',
                 'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker',
                 'labels']

# Assign the feature names to the dataset
data.columns = feature_names

"""> Attributes description



---

```
- Status of existing checking account, in Deutsche Mark.
- Duration in months
- Credit history (credits taken, paid back duly, delays, critical accounts)
- Purpose of the credit (car, television et al)
- Credit amount
- Status of savings account/bonds, in Deutsche Mark.
- Present employment, in number of years.
- Installment rate in percentage of disposable income
- Personal status (married, single,â€¦) and sex
- Other debtors / guarantors
- Present residence since X years
- Property (e.g. real estate)
- Age in years
- Other installment plans (banks, stores)
- Housing (rent, own et al)
- Number of existing credits at this bank
- Job
- Number of people being liable to provide maintenance for
- Telephone (yes, no)
- Foreign worker (yes, no)
```
"""

data.head()

"""### `Preprocessing the dataset : Data Aug, Feature Selection/Importance`"""

# Convert categorical variables to numerical using one-hot encoding
categorical_cols = ['status', 'credit_history', 'purpose', 'savings', 'employment_duration',
                    'statussex', 'other_debtors', 'property', 'other_installment_plans',
                    'housing', 'job', 'telephone', 'foreign_worker']
data = pd.get_dummies(data, columns=categorical_cols)

# labels: class 1(good credit)", "class 0(bad credit)
data.labels.replace([1,2], [1,0], inplace=True)

# Define the target variable and the features
target_col = 'labels'

features = data.drop(target_col, axis=1)

target = data[target_col]

sensitive_features = ['statussex', 'age']

"""### `Strategy 1: Applying data augmentation technique`"""

# Apply SMOTE : Synthetic Minority Oversampling Technique (SMOTE) is a statistical technique for increasing the number of cases in your dataset in a balanced way.
sm = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = sm.fit_resample(features , target)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

print("Before applying SMOTE technique \n")

unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

print("\n After applying SMOTE technique \n")

unique, counts = np.unique(y_resampled, return_counts=True)
print(dict(zip(unique, counts)))

"""### `Training the XGBoost Model`"""

# Define the XGBoost model
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'tree_method': 'hist',
    'max_depth': 8,
    'learning_rate': 0.1,
    'n_estimators': 1000,
    'seed': 42
}

# Train the XGBoost classifier
clf = xgb.XGBClassifier(**params)
clf.fit(X_train, y_train)

"""### `Testing the XGBoost Model`"""

# Make predictions on the test set
y_pred = clf.predict(X_test)

"""### `Evaluating the model`"""

# Evaluate the classifier's performance
print(classification_report(y_test, y_pred))

print("Accuracy of the model is: ", round(accuracy_score(y_test, y_pred), 2)*100,"%")

"""### `Applying fairness technique`"""

from sklearn.metrics import confusion_matrix

# TPR = True positive rate : a metric that measures the percentage of actual positives that are accurately identified.
def true_positive_rate(y_true, y_pred, sensitive_features, group):
    # Filter the instances belonging to the specified group
    group_indices = (sensitive_features == group)

    # Calculate the true positives (TP)
    tp = sum((y_true == 1) & (y_pred == 1) & group_indices)

    # Calculate the total positives (TP + FN) for the specified group
    total_positives = sum((y_true == 1) & group_indices)

    # Calculate the TPR for the specified group
    tpr = tp / total_positives if total_positives > 0 else 0

    return tpr

# Calculate the Equal Opportunity Difference
def equal_opportunity_difference(y_true, y_pred, sensitive_features, privileged_group, unprivileged_group):
    tpr_privileged = true_positive_rate(y_true, y_pred, sensitive_features, privileged_group)
    tpr_unprivileged = true_positive_rate(y_true, y_pred, sensitive_features, unprivileged_group)

    eod = tpr_privileged - tpr_unprivileged

    return eod


# Convert the sensitive features to a DataFrame
sensitive_data = pd.DataFrame(sensitive_features, columns=['statussex'])

# Combine the true labels and sensitive features into a single DataFrame
evaluation_data = pd.concat([y_test.reset_index(drop=True), sensitive_data.reset_index(drop=True)], axis=1)

# Group the evaluation data by the sensitive attribute
grouped_data = evaluation_data.groupby('statussex')

# Calculate Equal Opportunity Difference for each group
eod = equal_opportunity_difference(evaluation_data['labels'], y_pred, grouped_data, 1, 0)

# Print results
print("Equal Opportunity Difference:")
print(eod)

"""#### `In terms of fairness, achieving an Equal Opportunity Difference of zero is often considered desirable as it indicates that the model is providing fair treatment in terms of positive predictions across different groups. It suggests that the model's performance in identifying positive instances (e.g., detecting the positive outcome of a specific class) is consistent for both the privileged and unprivileged groups, without favoring one group over the other.`"""

# Get the predicted labels for the privileged and unprivileged groups
privileged = y_pred[(X_test['statussex_A91'] == 1) | (X_test['statussex_A93'] == 1) | (X_test['statussex_A94'] == 1) & (X_test['age'] > 18)]
unprivileged = y_pred[(X_test['statussex_A92'] == 1) & (X_test['age'] > 18)]

# Calculate the proportions of positive predictions for each group
privileged_proportion = np.mean(privileged)
unprivileged_proportion = np.mean(unprivileged)

# Calculate the Statistical Parity Difference
statistical_parity_difference = privileged_proportion - unprivileged_proportion

# Print the Statistical Parity Difference
print(f"Statistical Parity Difference: {statistical_parity_difference:.4f}")

"""### `Explanability of model and predictions`"""

!pip install shap

"""### `SHAP and LIME techniques`

---


```
SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) are two popular techniques for explaining the predictions of machine learning models.

SHAP values provide an explanation of the contribution of each feature to a specific prediction.  
Positive SHAP values indicate that the feature positively contributes to the prediction, while negative values indicate a negative contribution. The magnitude of the SHAP value represents the strength of the contribution.

LIME, on the other hand, focuses on explaining individual predictions. It approximates the behavior of a complex model by creating a local, interpretable model around the prediction of interest.

LIME generates a set of local perturbations to the input and observes how the model predictions change. It then assigns importance weights to the features based on their impact on the local model's predictions.


The choice between SHAP and LIME depends on the specific requirements of the use case and the level of interpretability desired.

```


"""

import shap

# Create an explainer object using the XGBoost model and the training data
explainer = shap.Explainer(clf, X_train)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# Summarize the SHAP values
shap.summary_plot(shap_values, X_test)

!pip install lime

!pip install shap

import lime
import lime.lime_tabular

# Create an explainer object using the XGBoost model and the feature names
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns)

# Select a specific instance for explanation
instance_idx = 5
instance = X_test.iloc[instance_idx]

# Generate LIME explanation for the selected instance
explanation = explainer.explain_instance(instance.values, clf.predict_proba, num_features=len(X_train.columns))

# Print the explanation
explanation.show_in_notebook()

"""#### `Below LIME explanation for a specific instance, shows the contribution of each feature towards the predicted probability.`"""

import lime
import lime.lime_tabular

# Create an explainer object using the XGBoost model and the feature names
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns)

# Select a specific instance for explanation
instance_idx = 10
instance = X_test.iloc[instance_idx]

# Generate LIME explanation for the selected instance
explanation = explainer.explain_instance(instance.values, clf.predict_proba, num_features=len(X_train.columns))

# Print the explanation
explanation.show_in_notebook()

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from lime.lime_text import LimeTextExplainer

# Sample training data
texts = [ "I love the beaches at Miami!!", "This movie is terrible", "The sounds so exciting!", "The plot was not good", "We have so many attendees, be it developers, CEO's, datascientists, researchers at the PyCon conference!",
         "I love the night-life at Estonia!"]
labels = [1, 0, 1, 0, 1, 1]

# Vectorize the data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# Train a logistic regression classifier
model = make_pipeline(vectorizer, LogisticRegression())
model.fit(texts, labels)

# Text to explain
text_to_explain = "So exciting to be presenting at the PyCon Estonia!!"
#text_to_explain = "I do not like this movie"

# LIME explanation
explainer = LimeTextExplainer(class_names=["Negative", "Positive"])
exp = explainer.explain_instance(text_to_explain, model.predict_proba, num_features=5)
exp.show_in_notebook(text=True)

!pip install lime

